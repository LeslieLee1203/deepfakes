{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "df_lstm_features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Video 資料位置**\n",
        "\n",
        "- https://drive.google.com/file/d/1-9ngKHZ5QN2bb6kTRgKGNqjAguwSbtix/view?usp=sharing\n",
        "\n",
        "\n",
        "**CNN Weights**\n",
        "\n",
        "- https://drive.google.com/file/d/17Uem7LOIMwNvLSbkQSOEYc5cnghcbnl8/view?usp=sharing"
      ],
      "metadata": {
        "id": "Q_xkO_y2yAZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pAO_KDrdyzlu",
        "outputId": "11bbea5d-94c4-413a-a4b9-677712f7e274"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xh8q_uMZtH4o"
      },
      "outputs": [],
      "source": [
        "#@title 從Google Drive 匯入影片資料\n",
        "train_data_path = '/content/drive/MyDrive/aidataset/df_train.zip' #@param {type:\"string\"}\n",
        "cmd  = f'cp {train_data_path} ./df_train.zip'\n",
        "! $cmd\n",
        "! unzip df_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 從Google Drive 匯入CNN Weights資料\n",
        "train_data_path = '/content/drive/MyDrive/aidataset/cnn_weights.hdf5' #@param {type:\"string\"}\n",
        "cmd  = f'cp {train_data_path} ./cnn_weights.hdf5'\n",
        "! $cmd\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eX630h12i1rH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 安裝 facenet_pytorch\n",
        "! pip install facenet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "aSmq4pfh-jEI",
        "outputId": "82557733-a1ce-42f1-bef6-c07b63b68692"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet_pytorch\n",
            "  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.21.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2021.10.8)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision->facenet_pytorch) (3.10.0.2)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 載入 CNN 模型\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "from os.path import exists\n",
        "from os import makedirs\n",
        "import tensorflow.compat.v1.keras.backend as K\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def cnn_model(model_name, img_size, weights):\n",
        "    \"\"\"\n",
        "    Model definition using Xception net architecture\n",
        "    \"\"\"\n",
        "    input_size = (img_size, img_size, 3)\n",
        "    if model_name == \"xception\":\n",
        "        baseModel = Xception(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"iv3\":\n",
        "        baseModel = InceptionV3(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"irv2\":\n",
        "        baseModel = InceptionResNetV2(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "    elif model_name == \"resnet\":\n",
        "        baseModel = ResNet50(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(img_size, img_size, 3)\n",
        "        )\n",
        "\n",
        "\n",
        "    headModel = baseModel.output\n",
        "    headModel = GlobalAveragePooling2D()(headModel)\n",
        "    headModel = Dense(512, activation=\"relu\", kernel_initializer=\"he_uniform\", name=\"fc1\")(\n",
        "        headModel\n",
        "    )\n",
        "    headModel = Dropout(0.4)(headModel)\n",
        "\n",
        "    predictions = Dense(\n",
        "        2,\n",
        "        activation=\"softmax\",\n",
        "        kernel_initializer=\"he_uniform\")(\n",
        "        headModel\n",
        "    )\n",
        "    model = Model(inputs=baseModel.input, outputs=predictions)\n",
        "\n",
        "    model.load_weights(weights + \".hdf5\")\n",
        "    print(\"Weights loaded...\")\n",
        "    model_lstm = Model(\n",
        "        inputs=baseModel.input,\n",
        "        outputs=model.get_layer(\"fc1\").output\n",
        "    )\n",
        "\n",
        "    for layer in baseModel.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    optimizer = Nadam(\n",
        "        lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004\n",
        "    )\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model_lstm\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "El6KwGfA8yCl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 利用CNN 擷取影片特徵\n",
        "from facenet_pytorch import MTCNN\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from glob import glob\n",
        "#listdir()\n",
        "# Create face detector\n",
        "mtcnn = MTCNN(\n",
        "    margin=40,\n",
        "    select_largest=False,\n",
        "    post_process=False,\n",
        "    device=\"cuda:0\"\n",
        ")\n",
        "\n",
        "train_dir = \"./train/\"\n",
        "sub_directories = listdir(train_dir)\n",
        "\n",
        "videos = []\n",
        "\n",
        "for i in sub_directories:\n",
        "    videos += glob(join(train_dir, i, \"*.mp4\"))\n",
        "model_choice = 'xception' #@param [\"xception\", \"iv3\", \"irv2\", \"resnet\"]\n",
        "# Loading model for feature extraction\n",
        "model = cnn_model(\n",
        "    model_name=model_choice,\n",
        "    img_size=160, \n",
        "    weights = 'cnn_weights'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "uqDdrx1Y-XVM",
        "outputId": "4a7983d2-da54-48df-a82b-1af27f01430b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "83697664/83683744 [==============================] - 1s 0us/step\n",
            "Weights loaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/nadam.py:73: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Nadam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 準備好 Feautre 並存成npy 檔案\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "features = []\n",
        "counter = 0\n",
        "labels = []\n",
        "\n",
        "for video in videos:\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    labels += [int(video.split(\"/\")[-2])]\n",
        "\n",
        "    batches = []\n",
        "\n",
        "    while cap.isOpened() and len(batches) < 25:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        h, w, _ = frame.shape\n",
        "        if h >= 1080 and w >= 1920:\n",
        "            frame = cv2.resize(\n",
        "                frame,\n",
        "                (640, 480),\n",
        "                interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = Image.fromarray(frame)\n",
        "        face = mtcnn(frame)\n",
        "\n",
        "        try:\n",
        "            face = face.permute(1, 2, 0).int().numpy()\n",
        "            batches.append(face)\n",
        "        except AttributeError:\n",
        "            print(\"Image Skipping\")\n",
        "\n",
        "    cap.release()\n",
        "    batches = np.array(batches).astype(\"float32\")\n",
        "    batches /= 255\n",
        "\n",
        "    # fc layer feature generation\n",
        "    predictions = model.predict(batches)\n",
        "\n",
        "    features += [predictions]\n",
        "\n",
        "    if counter % 50 == 0:\n",
        "        print(\"Number of videos done:\", counter)\n",
        "    counter += 1\n",
        "\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(features.shape, labels.shape)\n",
        "\n",
        "np.save(\"lstm_40fpv_data.npy\", features)\n",
        "np.save(\"lstm_40fpv_labels.npy\", labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "nLWTu_Zq-9EQ",
        "outputId": "27322e1f-c2eb-413b-ab7e-316785eeb055"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of videos done: 0\n",
            "Number of videos done: 50\n",
            "Number of videos done: 100\n",
            "Number of videos done: 150\n",
            "(165, 25, 512) (165,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 將打包檔案搬回Google Drive\n",
        "dst_path = '/content/drive/MyDrive/aidataset/' #@param {type:\"string\"}\n",
        "cmd  = f'cp lstm_*.npy {dst_path}'\n",
        "! $cmd"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cLtJBzEvk38f"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}